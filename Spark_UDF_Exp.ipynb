{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import time_uuid\n",
    "from datetime import datetime\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"PySpark Cassandra Test\").set(\"spark.cassandra.connection.host\", \"192.168.56.101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.stop()\n",
    "sc = SparkContext(master=\"local[*]\", conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext.sql(\"\"\"CREATE TEMPORARY TABLE api_events \\\n",
    "                  USING org.apache.spark.sql.cassandra \\\n",
    "                  OPTIONS ( table \"api_events\", \\\n",
    "                            keyspace \"simpl_events_production\", \\\n",
    "                            cluster \"rootCSSCluster\", \\\n",
    "                            pushdown \"true\") \\\n",
    "              \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.sql('SELECT bucket_id, cast(event_timestamp AS STRING), event_name, payload FROM api_events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def timestamp_to_unix_timestamp(date_time):\n",
    "    t = time_uuid.TimeUUID(date_time)\n",
    "    return int(t.get_timestamp()*1000000)\n",
    "\n",
    "def toCSVLine(data):\n",
    "  return ','.join(str(d) for d in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(bucket_id,StringType,true),StructField(event_timestamp,StringType,true),StructField(event_name,StringType,true),StructField(payload,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "timestamp_udf = udf(timestamp_to_unix_timestamp, LongType())\n",
    "print df.schema\n",
    "df = df.withColumn(\"unix_timestamp\", timestamp_udf(df[\"event_timestamp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lines = df.map(toCSVLine)\n",
    "# df.saveAsTable('spark_udf_with_payload')\n",
    "# df.save('spark_udf_with_payload')\n",
    "# df.rdd.saveAsTextFile(\"file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print df.select('unix_timestamp').distinct().count()\n",
    "# print df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(bucket_id=u'date_2016_02_10'),\n",
       " Row(bucket_id=u'date_2016_02_11'),\n",
       " Row(bucket_id=u'date_2016_02_12'),\n",
       " Row(bucket_id=u'date_2016_02_13'),\n",
       " Row(bucket_id=u'date_2016_02_14'),\n",
       " Row(bucket_id=u'date_2016_02_15'),\n",
       " Row(bucket_id=u'date_2016_02_16'),\n",
       " Row(bucket_id=u'date_2016_02_17'),\n",
       " Row(bucket_id=u'date_2016_02_18'),\n",
       " Row(bucket_id=u'date_2016_02_19'),\n",
       " Row(bucket_id=u'date_2016_02_20'),\n",
       " Row(bucket_id=u'date_2016_02_21'),\n",
       " Row(bucket_id=u'date_2016_02_22'),\n",
       " Row(bucket_id=u'date_2016_02_23'),\n",
       " Row(bucket_id=u'date_2016_02_24'),\n",
       " Row(bucket_id=u'date_2016_02_25'),\n",
       " Row(bucket_id=u'date_2016_02_26'),\n",
       " Row(bucket_id=u'date_2016_02_27'),\n",
       " Row(bucket_id=u'date_2016_02_28'),\n",
       " Row(bucket_id=u'date_2016_02_29'),\n",
       " Row(bucket_id=u'date_2016_03_01'),\n",
       " Row(bucket_id=u'date_2016_03_02'),\n",
       " Row(bucket_id=u'date_2016_03_03'),\n",
       " Row(bucket_id=u'date_2016_03_04'),\n",
       " Row(bucket_id=u'date_2016_03_05'),\n",
       " Row(bucket_id=u'date_2016_03_06'),\n",
       " Row(bucket_id=u'date_2016_03_07'),\n",
       " Row(bucket_id=u'date_2016_03_08'),\n",
       " Row(bucket_id=u'date_2016_03_09'),\n",
       " Row(bucket_id=u'date_2016_02_08'),\n",
       " Row(bucket_id=u'date_2016_02_09'),\n",
       " Row(bucket_id=u'date_2016_03_10')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# writing the df into HDFS\n",
    "df.select('bucket_id').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
